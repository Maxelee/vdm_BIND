[TRAINING]
# DiT (Diffusion Transformer) Configuration
# Based on "Scalable Diffusion Models with Transformers" (Peebles & Xie, 2023)

# Model architecture
model = dit
dit_variant = DiT-B/4
# Options: DiT-S/4, DiT-S/8, DiT-B/4, DiT-B/8, DiT-L/4, DiT-L/8, DiT-XL/4, DiT-XL/8
# S=Small (384 hidden, 12 depth), B=Base (768 hidden, 12 depth)
# L=Large (1024 hidden, 24 depth), XL=Extra Large (1152 hidden, 28 depth)
# /4 or /8 = patch size

# Image size
img_size = 128

# Manual DiT params (override variant if specified)
# patch_size = 4
# hidden_size = 768
# depth = 12
# num_heads = 12
# mlp_ratio = 4.0

# Conditioning
n_params = 35
conditioning_channels = 1
large_scale_channels = 3

# Noise schedule (VDM)
gamma_min = -13.3
gamma_max = 5.0

# Training hyperparameters
learning_rate = 1e-4
weight_decay = 1e-5
batch_size = 32
max_epochs = 500
gradient_clip_val = 1.0

# Loss
loss_type = mse

# Scheduler
lr_scheduler = cosine
warmup_steps = 2000

# Sampling
n_sampling_steps = 256

# Regularization
dropout = 0.1

# Data
dataset = rotated2_128
train_samples = True
num_workers = 8

# Logging
log_dir = /mnt/home/mlee1/ceph/tb_logs/dit/
experiment_name = dit_base_p4

# Checkpointing
save_top_k = 3
every_n_epochs = 10

# Precision
precision = 16-mixed

# Validation
val_check_interval = 0.25
n_val_samples = 4
