[TRAINING]
# ==============================================================================
# DiT (Diffusion Transformer) Configuration
# ==============================================================================
# 
# Based on "Scalable Diffusion Models with Transformers" (Peebles & Xie, 2023)
#
# This config uses the DiT architecture instead of UNet, with VDM-style
# diffusion loss for fair comparison.
#
# Key features:
#   - Patch-based transformer (no convolutions)
#   - adaLN-Zero conditioning on timestep + parameters
#   - Cross-attention for spatial conditioning
#   - Pre-defined variants: DiT-S, DiT-B, DiT-L, DiT-XL
# ==============================================================================

seed = 8
dataset = IllustrisTNG
data_root = /mnt/home/mlee1/ceph/train_data_rotated2_128_cpu/train/
field = gas
boxsize = 6.25

# ==============================================================================
# TRAINING HYPERPARAMETERS
# ==============================================================================
# Match VDM/DSM settings for fair comparison

batch_size = 16
accumulate_grad_batches = 2
num_workers = 20
cropsize = 128
max_epochs = 200

# ==============================================================================
# LEARNING RATE
# ==============================================================================

learning_rate = 1e-4
weight_decay = 1e-5
lr_scheduler = cosine_warmup
gradient_clip_val = 1.0

# ==============================================================================
# DiT ARCHITECTURE
# ==============================================================================
# Variant format: DiT-{S,B,L,XL}/{patch_size}
# S=Small (384 hidden, 12 depth), B=Base (768 hidden, 12 depth)
# L=Large (1024 hidden, 24 depth), XL=Extra Large (1152 hidden, 28 depth)
# /4 or /8 = patch size (4 gives more patches = more compute)

dit_variant = DiT-B/4

# Manual override (uncomment to override variant settings)
# patch_size = 4
# hidden_size = 768
# depth = 12
# num_heads = 12
# mlp_ratio = 4.0

# Conditioning channels
# 1 for DM input, 3 for large-scale context (12.5, 25, 50 Mpc/h)
conditioning_channels = 1
large_scale_channels = 3

# Parameter conditioning (cosmological parameters)
use_param_conditioning = True
param_norm_path = /mnt/home/mlee1/Sims/IllustrisTNG_extras/L50n512/SB35/SB35_param_minmax.csv

# ==============================================================================
# QUANTILE NORMALIZATION
# ==============================================================================

quantile_path = /mnt/home/mlee1/vdm_BIND/data/quantile_normalizer_stellar.pkl
use_quantile_normalization = True

# ==============================================================================
# NOISE SCHEDULE (VDM-style)
# ==============================================================================

gamma_min = -13.3
gamma_max = 13.0

# ==============================================================================
# LOSS CONFIGURATION
# ==============================================================================

loss_type = mse

# Per-channel weights [DM, Gas, Stars]
channel_weights = 1.0, 1.0, 1.0

# ==============================================================================
# SAMPLING
# ==============================================================================

n_sampling_steps = 256

# ==============================================================================
# EMA (Exponential Moving Average)
# ==============================================================================

enable_ema = True
ema_decay = 0.9999
ema_update_after_step = 1000
ema_update_every = 1

# ==============================================================================
# EARLY STOPPING
# ==============================================================================

enable_early_stopping = True
early_stopping_patience = 300

# ==============================================================================
# GRADIENT MONITORING
# ==============================================================================

enable_gradient_monitoring = True
gradient_log_frequency = 100

# ==============================================================================
# TRAINING DATA
# ==============================================================================

limit_train_samples = None
limit_val_samples = None
limit_train_batches = 1.0

# ==============================================================================
# LOGGING
# ==============================================================================

tb_logs = /mnt/home/mlee1/ceph/tb_logs3
model_name = dit_base_p4
version = 1

# ==============================================================================
# SPEED OPTIMIZATIONS
# ==============================================================================
# precision: "32" (default), "16-mixed" (faster on modern GPUs), "bf16-mixed"
# compile_model: Use torch.compile for ~10-30% speedup (requires PyTorch 2.0+)
# Note: Using 32-bit precision to avoid NaN issues that occurred with 16-mixed

precision = 32
compile_model = False
