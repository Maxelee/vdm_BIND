[TRAINING]
# ==============================================================================
# DSM (Denoising Score Matching) with Custom UNet Configuration
# ==============================================================================
# 
# This config uses the same UNet architecture as VDM/Interpolant but with
# Denoising Score Matching (DSM) loss instead of VDM ELBO or flow matching.
#
# This allows fair comparison between:
#   - VDM: ELBO loss with learned gamma schedule
#   - DSM: MSE on noise prediction with VP-SDE schedule (this config)
#   - Interpolant: MSE on velocity prediction with linear interpolation
#
# Key features:
#   - Same architecture as VDM (Fourier features, cross-attention, FiLM)
#   - VP-SDE noise schedule (beta_min, beta_max)
#   - Simple DSM loss: || epsilon_hat - epsilon ||^2
#   - Optional SNR weighting to match VDM loss formulation
# ==============================================================================

seed = 8
dataset = IllustrisTNG
data_root = /mnt/home/mlee1/ceph/train_data_rotated2_128_cpu/train/
field = gas
boxsize = 6.25

# ==============================================================================
# TRAINING HYPERPARAMETERS
# ==============================================================================
# Match VDM/Interpolant settings for fair comparison

batch_size = 64
accumulate_grad_batches = 4
num_workers = 8
cropsize = 128
max_epochs = 250

# ==============================================================================
# LEARNING RATE
# ==============================================================================

learning_rate = 1e-4
weight_decay = 1e-5
lr_scheduler = cosine_warmup

# ==============================================================================
# ARCHITECTURE (same as VDM/Interpolant)
# ==============================================================================
# Uses the same UNet architecture for fair comparison.
# embedding_dim=96 matches VDM Clean.

embedding_dim = 96
n_blocks = 5
norm_groups = 8
n_attention_heads = 8

# Conditioning channels
# 1 for DM input, 3 for large-scale context (12.5, 25, 50 Mpc/h)
conditioning_channels = 1
large_scale_channels = 3

# Fourier features and attention
use_fourier_features = True
fourier_legacy = False
add_attention = True

# Parameter conditioning (cosmological parameters via FiLM modulation)
use_param_conditioning = True
param_norm_path = /mnt/home/mlee1/Sims/IllustrisTNG_extras/L50n512/SB35/SB35_param_minmax.csv

# ==============================================================================
# QUANTILE NORMALIZATION
# ==============================================================================

quantile_path = /mnt/home/mlee1/vdm_BIND/data/quantile_normalizer_stellar.pkl
use_quantile_normalization = True

# ==============================================================================
# NOISE SCHEDULE (VP-SDE)
# ==============================================================================
# VP-SDE uses a linear beta schedule:
#   beta(t) = beta_min + t * (beta_max - beta_min)
#
# These values match common DDPM settings:
#   beta_min=0.1, beta_max=20.0 (from Song et al., 2021)

beta_min = 0.1
beta_max = 20.0

# ==============================================================================
# LOSS CONFIGURATION
# ==============================================================================

# SNR weighting: weight loss by d(log_snr)/dt like VDM
# This helps the model learn equally across all noise levels
use_snr_weighting = True

# Per-channel weights [DM, Gas, Stars]
# Can increase stellar weight if needed
channel_weights = 1.0,1.0,1.0

# ==============================================================================
# SAMPLING
# ==============================================================================
# DSM/DDPM typically needs more steps than flow matching

n_sampling_steps = 250

# ==============================================================================
# EMA (Exponential Moving Average)
# ==============================================================================
# EMA helps stabilize sampling quality

enable_ema = True
ema_decay = 0.9999
ema_update_after_step = 0
ema_update_every = 1

# ==============================================================================
# EARLY STOPPING
# ==============================================================================

enable_early_stopping = True
early_stopping_patience = 300

# ==============================================================================
# GRADIENT MONITORING
# ==============================================================================

enable_gradient_monitoring = True
gradient_log_frequency = 100

# ==============================================================================
# LOGGING
# ==============================================================================

tb_logs = /mnt/home/mlee1/ceph/tb_logs2/
model_name = dsm_3ch
version = 0

# ==============================================================================
# SPEED OPTIMIZATIONS
# ==============================================================================
# precision: "32" (default), "16-mixed" (faster on modern GPUs), "bf16-mixed"
# compile_model: Use torch.compile for ~10-30% speedup (requires PyTorch 2.0+)

precision = 16-mixed
compile_model = True
