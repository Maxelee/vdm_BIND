[TRAINING]
# ==============================================================================
# DDPM/Score Model Configuration
# ==============================================================================
# 
# This config uses the score_models package to train diffusion models
# with Denoising Score Matching (DSM) instead of VDM ELBO.
#
# Key differences from VDM:
#   - Uses VP-SDE (beta_min/beta_max) instead of learned gamma schedule
#   - DSM loss instead of diffusion + latent + reconstruction
#   - Compatible with NCSNpp and DDPM architectures
# ==============================================================================

seed = 8
dataset = IllustrisTNG
data_root = /mnt/home/mlee1/ceph/train_data_rotated2_128_cpu/train/
field = gas
boxsize = 6.25 

# ==============================================================================
# TRAINING HYPERPARAMETERS
# ==============================================================================
# NOTE: NCSNpp with attention is memory-intensive due to activations
# With DDP on 4x A100-40GB, batch_size=16 means 4 per GPU
# If still OOM, try batch_size=8 or disable attention
#
# Effective batch size = batch_size * accumulate_grad_batches * num_gpus
# With batch_size=16, accumulate=4, 4 GPUs: effective=256

batch_size = 16
accumulate_grad_batches = 4
num_workers = 8
cropsize = 128
max_epochs = 100

# ==============================================================================
# LEARNING RATE
# ==============================================================================

learning_rate = 1e-4
lr_scheduler = cosine_warmup

# ==============================================================================
# ARCHITECTURE
# ==============================================================================
# Options: ncsnpp, ddpm
# ncsnpp: Yang Song's NCSN++ from "Score-Based Generative Modeling through SDEs"
# ddpm: Jonathan Ho's DDPM architecture

architecture = ncsnpp

# Base number of features
nf = 96

# Channel multipliers for each resolution level
# For 128x128 input: 128 -> 64 -> 32 -> 16 -> 8
ch_mult = 1,2,4,8

# Number of residual blocks per resolution (DDPM only)
num_res_blocks = 2

# Use attention blocks
attention = True

# Dropout rate
dropout = 0.1

# ==============================================================================
# SDE CONFIGURATION
# ==============================================================================
# VP-SDE (Variance Preserving): Like DDPM, uses beta schedule
# VE-SDE (Variance Exploding): Like NCSN, uses sigma schedule

sde = vp

# VP-SDE parameters (used when sde = vp)
beta_min = 0.1
beta_max = 20.0

# VE-SDE parameters (used when sde = ve)
sigma_min = 0.01
sigma_max = 50.0

# ==============================================================================
# CONDITIONING
# ==============================================================================

# Large-scale conditioning channels (in addition to DM condition)
large_scale_channels = 3

# Use cosmological/astrophysical parameters for vector conditioning
# If True, the 35 parameters from the training data will be passed to the model
use_param_conditioning = True

# Number of parameters in the conditions vector
n_params = 35

# ==============================================================================
# EMA (EXPONENTIAL MOVING AVERAGE)
# ==============================================================================

enable_ema = True
ema_decay = 0.9999
ema_update_after_step = 1000
ema_update_every = 1

# ==============================================================================
# STELLAR NORMALIZATION
# ==============================================================================

param_norm_path = /mnt/home/mlee1/Sims/IllustrisTNG_extras/L50n512/SB35/SB35_param_minmax.csv
quantile_path = /mnt/home/mlee1/vdm_BIND/data/quantile_normalizer_stellar.pkl
use_quantile_normalization = True

# ==============================================================================
# MONITORING
# ==============================================================================

enable_early_stopping = False
early_stopping_patience = 50
enable_gradient_monitoring = True
gradient_log_frequency = 50

# ==============================================================================
# TRAINING DATA
# ==============================================================================

# Set to a number for testing (e.g., 1000), None for full dataset
# NOTE: For production training, set to None to use full dataset
# Score-based models require extensive training (100k+ steps)
limit_train_samples = 10000
limit_val_samples = 1000
limit_train_batches = 1.0

# ==============================================================================
# LOGGING
# ==============================================================================

model_name = ddpm_ncsnpp_vp
tb_logs = /mnt/home/mlee1/ceph/tb_logs3
version = 0

# ==============================================================================
# SPEED OPTIMIZATIONS
# ==============================================================================
# precision: "32" (default), "16-mixed" (faster on modern GPUs), "bf16-mixed"
# compile_model: Use torch.compile for ~10-30% speedup (requires PyTorch 2.0+)
# NOTE: compile_model disabled for NCSNPP - causes NaN with mixed precision

precision = 16-mixed
compile_model = False
