[TRAINING]
# ==============================================================================
# Stochastic Interpolant / Flow Matching Configuration
# ==============================================================================
# 
# This config uses STOCHASTIC interpolants (flow matching) for the DMO -> Hydro
# mapping, following the approach in BaryonBridge (Sadr et al.).
#
# Key differences from deterministic interpolant:
#   - Adds noise σ during the interpolation path: x_t = (1-t)x_0 + t*x_1 + σ*sqrt(t(1-t))*ε
#   - Can improve sample diversity at the cost of some convergence speed
#   - sigma=0.1 is a conservative choice; can increase to 0.2-0.5 for more diversity
# ==============================================================================

seed = 8
dataset = IllustrisTNG
data_root = /mnt/home/mlee1/ceph/train_data_rotated2_128_cpu/train/
field = gas
boxsize = 6.25

# ==============================================================================
# TRAINING HYPERPARAMETERS
# ==============================================================================
# Flow matching is often more stable than diffusion, allowing larger batch sizes
# and faster convergence.
#
# Effective batch size = batch_size * accumulate_grad_batches * num_gpus
# With batch_size=16, accumulate=4, 4 GPUs: effective=256

batch_size = 128
accumulate_grad_batches = 1
num_workers = 20
cropsize = 128
max_epochs = 200

# ==============================================================================
# LEARNING RATE
# ==============================================================================

learning_rate = 1e-4
weight_decay = 1e-5
lr_scheduler = cosine_warmup

# ==============================================================================
# ARCHITECTURE
# ==============================================================================
# Uses the same UNet architecture as VDM, but for velocity prediction
# instead of noise prediction.

embedding_dim = 96
n_blocks = 5
norm_groups = 8
n_attention_heads = 8

# Conditioning channels
# 1 for DM input, 3 for large-scale context (12.5, 25, 50 Mpc/h)
conditioning_channels = 1
large_scale_channels = 3

# Fourier features and attention
use_fourier_features = True
fourier_legacy = False
add_attention = True

# Parameter conditioning (cosmological parameters via FiLM modulation)
# The 35 cosmological/astrophysical parameters are embedded and used to
# modulate network features via FiLM (Feature-wise Linear Modulation)
use_param_conditioning = True
param_norm_path = /mnt/home/mlee1/Sims/IllustrisTNG_extras/L50n512/SB35/SB35_param_minmax.csv

# ==============================================================================
# QUANTILE NORMALIZATION
# ==============================================================================

quantile_path = /mnt/home/mlee1/vdm_BIND/data/quantile_normalizer_stellar.pkl
use_quantile_normalization = True

# ==============================================================================
# STOCHASTIC INTERPOLANT PARAMETERS
# ==============================================================================

# x0_mode: How to initialize the source distribution
#   - 'zeros': Start from zeros (recommended, simpler)
#   - 'noise': Start from Gaussian noise (like diffusion)
#   - 'dm_copy': Start from DM condition copied to all channels
#                This is physically motivated: learn the DMO → Hydro correction
#                Large-scale channels are still used as conditioning for the velocity model
x0_mode = dm_copy

# Stochastic interpolant: Add noise during interpolation
# sigma controls the amount of stochasticity in the interpolation path
# x_t = (1-t)x_0 + t*x_1 + sigma*sqrt(t(1-t))*noise
# sigma=0.1 is conservative, can increase to 0.2-0.5 for more diversity
use_stochastic_interpolant = True
sigma = 0.1

# Number of sampling steps for ODE integration
# Stochastic interpolants may benefit from slightly more steps
n_sampling_steps = 50

# ==============================================================================
# EMA (Exponential Moving Average)
# ==============================================================================
# EMA helps stabilize sampling quality

enable_ema = True
ema_decay = 0.9999
ema_update_after_step = 0
ema_update_every = 1

# ==============================================================================
# EARLY STOPPING
# ==============================================================================

enable_early_stopping = True
early_stopping_patience = 300

# ==============================================================================
# GRADIENT MONITORING
# ==============================================================================

enable_gradient_monitoring = True
gradient_log_frequency = 100

# ==============================================================================
# TRAINING DATA
# ==============================================================================

limit_train_samples = None
limit_val_samples = None
limit_train_batches = 1.0

# ==============================================================================
# LOGGING
# ==============================================================================

tb_logs = /mnt/home/mlee1/ceph/tb_logs3
model_name = stochastic_interpolant_3ch
version = 0

# ==============================================================================
# SPEED OPTIMIZATIONS
# ==============================================================================
# precision: "32" (default), "16-mixed" (faster on modern GPUs), "bf16-mixed"
# compile_model: Use torch.compile for ~10-30% speedup (requires PyTorch 2.0+)

precision = 16-mixed
compile_model = True
