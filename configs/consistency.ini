[TRAINING]
# ==============================================================================
# Consistency Model Configuration
# ==============================================================================
# 
# This config implements Consistency Models (Song et al., 2023) for the 
# DMO -> Hydro mapping.
#
# Key features:
#   - Single-step or few-step sampling with diffusion-quality results
#   - f_θ(x_t, t) = x_0 for all t (maps any noisy point to clean data)
#   - Self-consistency training (CT) from scratch
#   - Optional denoising pre-training for warm-up
#
# Advantages:
#   - MUCH faster sampling (1-4 steps vs 250-1000 for diffusion)
#   - Maintains high quality with proper training
#   - Trade-off: more steps = better quality
#
# Reference:
#   Song et al. (2023) "Consistency Models" https://arxiv.org/abs/2303.01469
# ==============================================================================

seed = 8
dataset = IllustrisTNG
data_root = /mnt/home/mlee1/ceph/train_data_rotated2_128_cpu/train/
field = gas
boxsize = 6.25

# ==============================================================================
# TRAINING HYPERPARAMETERS
# ==============================================================================
# Consistency training can be more sensitive to hyperparameters than diffusion.
# Start with smaller batch size and careful learning rate.

batch_size = 32
accumulate_grad_batches = 8
num_workers = 10
cropsize = 128
max_epochs = 300

# ==============================================================================
# LEARNING RATE
# ==============================================================================
# Consistency models benefit from careful LR scheduling.
# Start with smaller LR than diffusion models.

learning_rate = 5e-5
weight_decay = 1e-5
lr_scheduler = cosine_warmup

# ==============================================================================
# ARCHITECTURE
# ==============================================================================
# Uses same UNet architecture as VDM/DDPM/Interpolant

embedding_dim = 96
n_blocks = 5
norm_groups = 8
n_attention_heads = 8

# Conditioning channels
conditioning_channels = 1
large_scale_channels = 3

# Fourier features and attention
use_fourier_features = True
fourier_legacy = False
add_attention = True

# Parameter conditioning
use_param_conditioning = True
param_norm_path = /mnt/home/mlee1/Sims/IllustrisTNG_extras/L50n512/SB35/SB35_param_minmax.csv

# ==============================================================================
# QUANTILE NORMALIZATION
# ==============================================================================

quantile_path = /mnt/home/mlee1/vdm_BIND/data/quantile_normalizer_stellar.pkl
use_quantile_normalization = True

# ==============================================================================
# CONSISTENCY MODEL PARAMETERS
# ==============================================================================

# Number of sampling steps (1 for single-step, 2-4 for quality)
# Single-step is the main advantage of consistency models!
n_sampling_steps = 1

# Noise schedule parameters
# σ_min: Minimum noise (near clean data)
# σ_max: Maximum noise (pure noise)
# These affect the diffusion trajectory endpoints
sigma_min = 0.002
sigma_max = 80.0

# σ_data: Standard deviation of training data
# Used for skip connection scaling in consistency function
# Should roughly match the std of normalized training data
sigma_data = 0.5

# Consistency Training (CT) parameters
# ct_n_steps: Number of discretization steps for CT loss
# More steps = more precise but slower training
ct_n_steps = 18

# EMA decay for target network in CT
# Higher = more stable but slower adaptation
ct_ema_decay = 0.9999

# ==============================================================================
# DENOISING PRE-TRAINING
# ==============================================================================
# Optional warm-up with standard denoising loss before CT
# This helps stabilize early training

use_denoising_pretraining = True
denoising_warmup_epochs = 20

# ==============================================================================
# EMA FOR MODEL WEIGHTS
# ==============================================================================
# Separate from CT target EMA - for final model quality
# Note: Consistency models already have internal EMA for target network

enable_ema = False
ema_decay = 0.9999
ema_update_after_step = 0
ema_update_every = 1

# ==============================================================================
# EARLY STOPPING
# ==============================================================================

enable_early_stopping = True
early_stopping_patience = 300

# ==============================================================================
# GRADIENT MONITORING
# ==============================================================================

enable_gradient_monitoring = True
gradient_log_frequency = 100

# ==============================================================================
# LOGGING
# ==============================================================================

tb_logs = /mnt/home/mlee1/ceph/tb_logs3
model_name = consistency_3ch
version = 0

# ==============================================================================
# SPEED OPTIMIZATIONS
# ==============================================================================
# precision: "32" (default), "16-mixed" (faster on modern GPUs), "bf16-mixed"
# compile_model: Use torch.compile for ~10-30% speedup (requires PyTorch 2.0+)

precision = 16-mixed
compile_model = False
